{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import ast\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifisering av tekster med et LSTM-nettverk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hente fritekstsvarene fra Toppoppgaver og de annoterte tekstene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_matrix(df):\n",
    "    labels = set()\n",
    "    for row in df[[\"label\",\"index\"]].values:\n",
    "        label = row[0]\n",
    "        for l in label:\n",
    "            labels.add(l)\n",
    "\n",
    "    # Creating matrix of labels\n",
    "    labels = list(labels)\n",
    "    for label in labels:\n",
    "        df[label] = 0\n",
    "\n",
    "    col_inds = df.columns.values\n",
    "    for index,val in enumerate(df[[\"label\"]].values):\n",
    "        labels = val[0]\n",
    "        for label in labels:\n",
    "            col_ind = np.where(col_inds == label)[0]\n",
    "            df.iloc[index,col_ind] = 1\n",
    "    \n",
    "    df.drop([\"Date Submitted\",\"Hva kom du hit for å gjøre\", \"raw_text\",\"label\",\"Hvor lang tid brukte du?\", \"Fikk du gjort det du kom hit for å gjøre\"], axis = 1, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../hjemmesnekk/labeled_data/\"\n",
    "\n",
    "\n",
    "# for tilgang til disse csv filene trengs tilgang til kubeflow-serveren vår.\n",
    "csv_path = \"toppoppgaver.csv\" \n",
    "labeled_csv_path = \"toppoppgaver_NYESTE.csv\"\n",
    "\n",
    "labels = pd.read_csv(data_path + \"toppoppgaver_NYESTE.csv\")\n",
    "labels['label'] = labels['label'].apply(lambda x: ast.literal_eval(x))\n",
    "labels = convert_to_matrix(labels)\n",
    "labels = labels.set_index(\"index\")\n",
    "    \n",
    "#topp = text_processer(TOPPOPPGAVER)\n",
    "#df = topp.get_preprocessed_data()\n",
    "df = pd.read_csv(data_path + csv_path)\n",
    "df = df.loc[labels.index.values,:]\n",
    "df.sort_index(axis = 0, inplace = True)\n",
    "labels.sort_index(axis = 0, inplace = True)\n",
    "labels_intent.sort_index(axis = 0, inplace = True)\n",
    "labels_bakgrunn.sort_index(axis = 0, inplace = True)\n",
    "\n",
    "# lemmatiserer og naivt retter skrivefeil\n",
    "df[\"raw_text\"] = df[\"raw_text\"].apply(lemmatisering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"raw_text\"]\n",
    "classes = labels.columns.values\n",
    "y = labels[classes].values\n",
    "# deler opp i trenings- og testsett\n",
    "texts_train, texts_test, y_train, y_test = train_test_split(texts, y, test_size=0.2, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokeniserer og lager embeddingmatrise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove-modellen er trent opp på tekster fra skriv-til-oss og fra Hotjar, og er tilgjengelig på kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 100\n",
    "vocab_size = 500\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(list(texts_train))\n",
    "tokenized_train = tokenizer.texts_to_sequences(texts_train)\n",
    "tokenized_test = tokenizer.texts_to_sequences(texts_test)\n",
    "X_train = pad_sequences(tokenized_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(tokenized_test, maxlen=maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(glove_path+'vectors_100d.txt'))\n",
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(vocab_size, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bygge LSTM-nettverket og trene det"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM_text_model(maxlen = maxlen, embed_dim = embed_dim, nb_word = nb_words, linear_layers = 3, linear_nodes = 200, linear_dropout = 0.1):\n",
    "    inp = Input(shape = (maxlen,))\n",
    "    x = Embedding(nb_words, embed_dim, weights=[embedding_matrix])(inp)\n",
    "    x = Bidirectional(LSTM(100, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    if linear_layers > 0:\n",
    "        for layer in range(linear_layers):\n",
    "            x = Dense(linear_nodes, activation=\"relu\")(x)\n",
    "            x = Dropout(linear_dropout)(x)\n",
    "    x = Dense(len(classes), activation = \"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x, name = \"lstm\")\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=3)])\n",
    "    return model\n",
    "\n",
    "model = create_LSTM_text_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbs = [tf.keras.callbacks.EarlyStopping(monitor = \"loss\", patience=5)]\n",
    "history = model.fit(X_train, y_train,batch_size=16, epochs=200, verbose = 2, validation_split=0.1, callbacks = cbs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lagre modellen\n",
    "model_path = '../models/NN/'\n",
    "\n",
    "model_json = model.to_json()\n",
    "from keras.models import model_from_json\n",
    "with open(model_path+\"model_80f.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(model_path+\"model_80f.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "json_file = open(model_path+'model_80f.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(model_path+\"model_80f.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.TopKCategoricalAccuracy(k=4)])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict([X_test], batch_size=1024, verbose=1)\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "# her er klasser med null i score tatt med, noe som betydelig senker snittet, derfor regner vi det ut selv hvor vi fjerner disse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([[1 if j>0.5 else 0 for j in i]for i in y_predict])\n",
    "mcm = multilabel_confusion_matrix(y_test, y_pred, samplewise=False)\n",
    "tn = mcm[:, 0, 0]\n",
    "tp = mcm[:, 1, 1]\n",
    "fn = mcm[:, 1, 0]\n",
    "fp = mcm[:, 0, 1]\n",
    "recall = tp / (tp + fn)\n",
    "precision = tn / (tn + fp) \n",
    "f1_score = [2 * (precision[i] * recall[i]) / (recall[i] + precision[i]) for i in range(len(classes))]\n",
    "unike = np.unique([i[1] for i in np.argwhere(y_test==1)], return_counts=True)\n",
    "indekser = list(unike[0])\n",
    "antall = list(unike[1])\n",
    "for i in range(len(classes)):\n",
    "    if i != indekser[i]:\n",
    "        indekser.insert(i,i)\n",
    "        antall.insert(i,0)\n",
    "        \n",
    "\n",
    "df_f1score = pd.DataFrame([np.round(recall, 2), np.round(precision, 2), map(str, antall), f1_score],index=[\"Recall\", \"Precision\", \"Antall\", \"f1_score\"], columns=classes).T                                                                                                            \n",
    "s = [i for i in df_f1score.f1_score if i > 0]\n",
    "print(sum(s)/len(s))\n",
    "df_f1score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klassifisere nye tekster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv(data_path + csv_path)\n",
    "df[\"raw_text\"] = df[\"raw_text\"].apply(lemmatisering)\n",
    "texts_to_predict = df[\"raw_text\"].values\n",
    "print(texts_to_predict)\n",
    "tokenized_texts = tokenizer.texts_to_sequences(texts_to_predict)\n",
    "X = pad_sequences(tokenized_texts, maxlen=maxlen)\n",
    "predicted = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "predicted[predicted > threshold] = 1\n",
    "predicted[predicted < threshold] = 0\n",
    "predicted_labels = []\n",
    "for i in range(predicted.shape[0]):\n",
    "    temp = []\n",
    "    for j in range(predicted.shape[1]):\n",
    "         if predicted[i,j] == 1: temp.append(classes[j])\n",
    "    predicted_labels.append(temp)\n",
    "df[\"Prediction\"] = predicted_labels\n",
    "df.to_pickle(\"../data/pickle/predicted_toppoppgaver/pred.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
