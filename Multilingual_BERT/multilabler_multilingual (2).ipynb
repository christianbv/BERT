{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_representation import InputExample, InputFeatures, truncate_pairs\n",
    "from transformers import AutoTokenizer, BertModel, BertForSequenceClassification\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers as t\n",
    "from sklearn import metrics \n",
    "import metrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.insert(1, \"../lib\")\n",
    "from text_processer import *\n",
    "\n",
    "\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import re\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametre:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_LABELS = False\n",
    "\n",
    "MODEL_PATH = \"../hjemmesnekk/multilingual\"\n",
    "MODEL_PATH_TRAINED = \"../hjemmesnekk/multilingual-trained/24.08_16_labels_8_epochs\"\n",
    "TOKENIZER_PATH = \"../hjemmesnekk/multilingual\"\n",
    "CONFIG_PATH = \"../hjemmesnekk/multilingual/config.json\"\n",
    "DO_LOWER = False\n",
    "TRAINING_SIZE = 0.8\n",
    "MAX_LEN = 512\n",
    "EPOCHS = 8\n",
    "BATCHES = 16\n",
    "LR = 5e-5 \n",
    "EPS = 1e-8\n",
    "RANDOM_SEED = 42\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "PRED_TRESH = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# Column names:\n",
    "\n",
    "INTENTS = [\"Sjekke\",\"Kontakte\",\"Registrere/lage\",\"Endre/Oppdatere\",\"Trekke/Fjerne\", \"Klage\",\"Finne\",\"Sende\",\"Søke\",\"Spørsmål\",\"Ikke oppgitt\", \"@@@@@@@\"]\n",
    "\n",
    "CAUSES = [\"Sykepenger\",\"Sykemelding\",\"Dagpenger\",\"Permittert\",\"Pensjon\",\"Skatt/Årsoppgave\",\"Meldekort\", \"Barn/Foreldre\", \"Ufør/AAP\", \"Informasjon\",  \n",
    "            # Dokumentasjon Labels:\n",
    "            \"Vedtak/Sak\", \"Dokumentasjon\", \"Skjema/søknad\", \"Melding/brev\", # DOKUMENTER \n",
    "            # MinSide labels:\n",
    "            \"Personopplysning\", # Kontonummer, endre egne personopplysninger osv \n",
    "            \"CV\",\"Arbeidssøker\", #Fjerne arbeidssøker = Tilbake i jobb, Registrere arbeidssøker = permittert/lignende, finne arbeidssøker = Arbeidsgivere som vil finne ansatte \n",
    "            \"Utbetaling\",\"Aktivitetsplan\", \n",
    "            \"Veileder\", \"Saksbehandler/NAV\",\n",
    "            \"Tekniske problemer\", \"Arbeidsgiver\", \"Annet språk\",\"Annet\", \"SPAM\"]\n",
    "\n",
    "# Argparse fungerer ikke av en eller annen grunn...\n",
    "#parser = ArgumentParser()\n",
    "#parser.add_argument(\" \")\n",
    "#parser.add_argument(\"--MODEL_PATH\", type=str, help = \"The path to the folder where the model is located\")\n",
    "#parser.add_argument(\"--TOKENIZER_PATH\", type = str, help = \"The path to the folder where the tokenizer is located\")\n",
    "#parser.add_argument(\"--EPOCHS\", type = int, default = 2, help = \"The number of epochs the model should perform (2 is often enough\")\n",
    "#parser.add_argument(\"--BATCHES\", type = int, default = 8, help = \"The number of batches for each epoch\")\n",
    "#parser.add_argument(\"--LR\", type = float, default = 5e-5, help = \"Learning rate\")\n",
    "#parser.add_argument(\"--MAX_LEN\", type = int, default= 512, help = \"The max. length of each text the model supports (in config file)\")\n",
    "#args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = t.AutoTokenizer.from_pretrained(TOKENIZER_PATH, strip_accents = False, do_lower_case = DO_LOWER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoder for henting av data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def load_labeled_data(cols_to_use, old = True)-> pd.DataFrame:\n",
    "    if old: \n",
    "        l_df = pd.read_csv(\"../hjemmesnekk/labeled_data/toppoppgaver.csv\").set_index(\"index\")\n",
    "    else: \n",
    "        l_df = pd.read_csv(\"../hjemmesnekk/labeled_data/toppoppgaver_NYESTE.csv\")\n",
    "        l_df = metrics.convert_to_matrix(l_df).set_index(\"index\")\n",
    "        print(l_df)\n",
    "        return\n",
    "    l_df = l_df[cols_to_use]\n",
    "    l_df[\"sum\"] = l_df.sum(axis = 1)\n",
    "    l_df = l_df[l_df[\"sum\"] == 1.0].drop(\"sum\", axis = 1)\n",
    "    return l_df\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "    Returns the labeled_data in its entirety\n",
    "\"\"\"\n",
    "def load_labeled_data(with_intents = True) -> pd.DataFrame:\n",
    "    if OLD_LABELS:\n",
    "        l_df = pd.read_csv(\"../hjemmesnekk/labeled_data/toppoppgaver.csv\").set_index(\"index\")\n",
    "        return l_df\n",
    "    else:\n",
    "        l_df = pd.read_csv(\"../hjemmesnekk/labeled_data/toppoppgaver_NYESTE.csv\")\n",
    "        l_df = metrics.convert_to_matrix(l_df).set_index(\"index\")\n",
    "        if not with_intents:\n",
    "            l_df.drop(INTENTS, axis = 1)\n",
    "        return l_df\n",
    "    \n",
    "\"\"\"\n",
    "    Filters the labeled_data on prespecified columns\n",
    "\"\"\"\n",
    "def filter_labeled_data(l_df: pd.DataFrame, cols: list = None) -> pd.DataFrame:\n",
    "    cols_to_use = cols if cols != None else l_df.columns.values\n",
    "    l_df = l_df[cols_to_use]\n",
    "    l_df[\"sum\"] = l_df.sum(axis = 1)\n",
    "    return l_df[l_df[\"sum\"] >= 1.0].drop(\"sum\", axis = 1)\n",
    "\n",
    "\n",
    "def load_data(preprocessed = True) -> pd.DataFrame:\n",
    "    t = text_processer(TOPPOPPGAVER)\n",
    "    if preprocessed:\n",
    "        return t.get_preprocessed_data()\n",
    "    else:\n",
    "        return t.get_cleaned_data()\n",
    "    \n",
    "def filter_on_labeled_data(df, labels) -> pd.DataFrame:\n",
    "    df = df.loc[l_df.index.values,:]\n",
    "    df.sort_index(axis = 0, inplace = True)\n",
    "    l_df.sort_index(axis = 0, inplace = True)\n",
    "    return df, l_df\n",
    "\n",
    "def drop_cols(df, cols:list) -> pd.DataFrame:\n",
    "    for col in df.columns.values:\n",
    "        if col not in cols: \n",
    "            raise TypeError(f'Col: {col} not in param {cols}')\n",
    "        assert col in cols\n",
    "    return df.drop(cols, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoder for parsing av data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Denne må legges inn en JSON fil etterhvert - og helst kjøres ved preprosessering av data.\n",
    "def lemmatize(x, do_lower = False):\n",
    "    x = x.lower() if do_lower else x\n",
    "    \n",
    "    lemmas = {\"vedrørende\":r'(\\bvedr[.]?)',\n",
    "              \"angående\":r'(\\bang.)',\n",
    "              \"angående\":r'(\\bang.)',\n",
    "              \"telefon\":r'(\\btlf[.]?)',\n",
    "              \"offentlig\":r'(\\boff.)',\n",
    "              \"konto\":r'(\\bkto.)',\n",
    "              \"kontonummer\":r'(\\bkontonr.)',\n",
    "              \"på grunn av \": r'(\\bpga.)',\n",
    "              \"nummer\":r'(\\bnr[.]?)',\n",
    "              \"registrere\":r'(\\breg.)',\n",
    "              \"tidligere\":r'(\\btidl[.]?)',\n",
    "              \"med vennlig hilsen\":r'(\\bmvh[.]?)',\n",
    "              \"utbetaling\": r'\\butbet+al[ingenrt]*|(\\butbet[.]?)',\n",
    "              \"dagpenger\": r'\\bdagpeng[aenr]{0,3}',\n",
    "              \"finne\": r'(\\bfinne[r]?)',\n",
    "              \"ferie\": r'\\bferie[ern]*',\n",
    "              \"arbeide\": r'\\barbeide[ret]{1,3}',\n",
    "              \"korona\": r'(\\bcorona[viruset]{0,6}\\Z)|(\\bkorona[viruset]{0,6}\\Z)|(\\bcovid[-19]{0,3}\\Z)',\n",
    "              \"lege\": r'(\\blegen?)',\n",
    "              \"fastlege\": r'(\\bfastlegen)',\n",
    "              \"melding\": r'(\\bmld)|(\\bmelding[enr]{0,3})',\n",
    "              \"lønn\": r'(\\blønn[nea]{0,3})',\n",
    "              \"pensjon\": r'(\\bpensjon[en]{0,2})|(\\bpension[en]{0,2})',\n",
    "              \"informasjon\": r'(\\binformasjon[en]{0,2})|(\\binfo[.]?)',\n",
    "              \"kontonummer\": r'(\\bkontonummer[et]{0,2})|(\\bkontonr[.]?)',\n",
    "              \"oppdatere\": r'(\\boppdatert)|(\\boppdatering)',\n",
    "              \"dokument\": r'(\\bdokument[ern]{0,3})|(\\bdok)(|\\bdoc)',\n",
    "              \"aktivitetsplan\": r'(\\bak?tivitets?plan[en]{0,3})',\n",
    "              \"endre\": r'(\\bendre)|(\\bendring)',\n",
    "              \"ettersende\": r'(\\bettersende?)|({\\bettersending})',\n",
    "              \"arbeidsledig\":r'(\\barb. ledig)|(\\barb.ledig)',\n",
    "              \"arbeids\":r'(\\barb.)',\n",
    "              \"for eksempel\":r'(\\bf.eks.)|(\\bf. eks.)',\n",
    "              \". \":r'(\\b\\n)',\n",
    "    }\n",
    "    for key in lemmas.keys():\n",
    "        x = re.sub(lemmas[key], key, x)\n",
    "    return x\n",
    "\n",
    "def lemmatize_data(df: pd.DataFrame, preprocessed = True) -> pd.DataFrame:\n",
    "    pred = lambda x: lemmatize(x)\n",
    "    df[\"sentences\"] = df.raw_text.apply(pred) if preprocessed else df[\"Hva kom du hit for å gjøre\"].apply(pred)\n",
    "    return df\n",
    "\n",
    "\"\"\"\n",
    "    Method to create inputExamples \n",
    "    Just instances with lemmatized texts and corresponding labels\n",
    "\"\"\"\n",
    "def createInputExamples(df, l_df, preprocessed = True):\n",
    "    inputExamples = []   \n",
    "    for i, val in enumerate(df[\"sentences\"].values):\n",
    "        labels = l_df.columns.values[np.where(l_df.values[i] == 1)]\n",
    "        inputExamples.append(InputExample(i, val[0:MAX_LEN], None, labels))\n",
    "    return inputExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metoder for oppbygging av dataen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Data():\n",
    "    \"\"\"\n",
    "        Takes in finished parsed data\n",
    "        Creates instances to be used when training the model. \n",
    "    \"\"\"\n",
    "    def __init__(self, inputExamples, labels: list, tokenizer: t.tokenization_bert, MAX_LEN: int):\n",
    "        self.inputExamples = inputExamples\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.MAX_LEN = MAX_LEN\n",
    "        self.label_map = {label : i for i,label in enumerate(labels)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.inputExamples)\n",
    "        \n",
    "    \"\"\" \n",
    "        Method to tokenize one instance in inputExamples.\n",
    "        Innebygget metode som tensordataset benytter seg av for å hente ut dataen\n",
    "    \"\"\"\n",
    "    def __getitem__(self, ind):\n",
    "        encoded_dict = self.tokenizer.encode_plus(self.inputExamples[ind].text_a,\n",
    "                                            add_special_tokens = True,\n",
    "                                            max_length = self.MAX_LEN,\n",
    "                                            pad_to_max_length = True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            truncation = True\n",
    "                                            #return_tensors = 'pt',\n",
    "        )\n",
    "        input_ids = encoded_dict['input_ids']\n",
    "        masks = encoded_dict['attention_mask']\n",
    "        token_type_ids = encoded_dict['token_type_ids']\n",
    "        \n",
    "        labels = [0]*len(self.label_map.keys())\n",
    "        for label in self.inputExamples[ind].labels:\n",
    "            labels[self.label_map.get(label)] = 1            \n",
    "            \n",
    "        return {\n",
    "            \"text\":self.inputExamples[ind].text_a,\n",
    "            \"input_ids\":torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"masks\":torch.tensor(masks, dtype = torch.long),\n",
    "            \"tokens\":torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"labels\":torch.tensor(labels, dtype = torch.float)\n",
    "        }  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppdeling av dataen og dataloaders for trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset: TensorDataset):\n",
    "    train_size = int(TRAINING_SIZE * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    print('{:>5,} training samples'.format(train_size))\n",
    "    print('{:>5,} validation samples'.format(val_size))\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def build_loaders(train_dataset, val_dataset):\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle = True, batch_size = BATCHES)\n",
    "    validation_dataloader = DataLoader(val_dataset, sampler = SequentialSampler(val_dataset), batch_size = BATCHES)\n",
    "    return train_dataloader, validation_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oppdeling av dataen, og dataloaders for trening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Takes in inputExamples\n",
    "    Creates trainingset and testset by sampling from the inputExamples\n",
    "    Returns trainingset and labelset\n",
    "\"\"\"\n",
    "def split_data2(inputExamples: list):\n",
    "    train_dataset = random.sample(inputExamples, int(TRAINING_SIZE*len(inputExamples)))\n",
    "    test_dataset = [inputExample for inputExample in inputExamples if inputExample not in train_dataset]\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "def createDataLoaders(train_dataset, test_dataset, labels, tokenizer):\n",
    "    training_set = Custom_Data(train_dataset, labels, tokenizer, MAX_LEN\n",
    "    )\n",
    "    \n",
    "    test_set = Custom_Data(test_dataset,labels,tokenizer, MAX_LEN\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(training_set, shuffle = True, batch_size = BATCHES)\n",
    "    test_loader = DataLoader(test_set, sampler = SequentialSampler(test_set), batch_size = BATCHES)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def texts_to_dataloader(texts, tokenizer, batch_size = 1): \n",
    "    data = []\n",
    "    for i, text in enumerate(texts):\n",
    "        tokenized_dict = tokenizer.encode_plus(text,\n",
    "                                               add_special_tokens = True,\n",
    "                                               max_length = MAX_LEN,\n",
    "                                               pad_to_max_length = True,\n",
    "                                               return_attention_mask = True,\n",
    "                                               truncation = True\n",
    "        )\n",
    "        \n",
    "        data.append({\n",
    "            \"id\":i,\n",
    "            \"text\":text,\n",
    "            \"input_ids\":torch.tensor(tokenized_dict['input_ids'], dtype=torch.long),\n",
    "            \"masks\":torch.tensor(tokenized_dict['attention_mask'], dtype = torch.long),\n",
    "            \"tokens\":torch.tensor(tokenized_dict['token_type_ids'], dtype = torch.long)            \n",
    "        })\n",
    "    \n",
    "    pred_loader = DataLoader(data, sampler = SequentialSampler(data), batch_size = batch_size)\n",
    "    return pred_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skall til multilabler BERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMultiLabler(nn.Module):\n",
    "    \"\"\"\n",
    "        Class for multilabler BERT.\n",
    "        Uses normal BertModel in bottom, a dropout layer and a linear classifier on top.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, config, isTrained):\n",
    "        super(BertMultiLabler, self).__init__()\n",
    "        if isTrained:\n",
    "            self.base_model = t.BertModel.from_pretrained(MODEL_PATH_TRAINED)\n",
    "        else:\n",
    "            self.base_model = t.BertModel.from_pretrained(MODEL_PATH)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, NUM_LABELS)\n",
    "\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, masks, tokens):\n",
    "        _, model_output = self.base_model(input_ids, attention_mask = masks, token_type_ids = tokens) # Model returns two params\n",
    "        dropout_output = self.dropout(model_output)\n",
    "        output = self.classifier(dropout_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tapsfunksjoner og optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    return nn.BCEWithLogitsLoss()(outputs, targets)\n",
    "\n",
    "def get_optimizer(model, o1 = False):\n",
    "    if o1:\n",
    "        return torch.optim.Adam(\n",
    "            params = model.parameters(),\n",
    "            lr = LR\n",
    "        )\n",
    "    else:\n",
    "        return AdamW(\n",
    "            model.parameters(), \n",
    "            lr = LR,\n",
    "            eps = EPS\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kjører alt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()\n",
    "filter_cols = None\n",
    "l_df = load_labeled_data()\n",
    "print(l_df.sum(axis=0))\n",
    "cols_to_use_old = [\"Finne info/oversikt\", \"Ettersende dokumentasjon\", \"Permittert\", \"Sjekke/endre kontonr\", \"Finne skjema/dokumenter\",\"Oppdatere/lage cv\", \"Pensjon\", \"Ukjent\",\"Dagpenger\"]\n",
    "intent_cols = [\"Trekke/Fjerne\",\"Klage\",\"Spørsmål\",\"Finne\",\"Registrere/lage\",\"Endre/Oppdatere\",\"Søke\",\"Sjekke\",\"Sende\",\"@@@@@@@\",\"Ikke oppgitt\"]\n",
    "cols_to_use = [\"Annet\",\"Pensjon\",\"Dagpenger\",\"Skjema/søknad\",\"Utbetaling\",\"Dokumentasjon\",\"Tekniske problemer\",\"Permittert\",\"Personopplysning\",\"SPAM\",\"Informasjon\",\"Skatt/Årsoppgave\", \"Arbeidssøker\",\"Sykepenger\",\"Sykemelding\",\"CV\"]\n",
    "l_df = filter_labeled_data(l_df, cols_to_use)\n",
    "NUM_LABELS = len(l_df.columns.values)\n",
    "df, l_df = filter_on_labeled_data(df, l_df)\n",
    "l_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_data()\n",
    "#cols_to_use = [\"Finne info/oversikt\", \"Ettersende dokumentasjon\", \"Permittert\", \"Sjekke/endre kontonr\", \"Finne skjema/dokumenter\",\"Oppdatere/lage cv\"]\n",
    "#cols_to_use = [\"Finne info/oversikt\", \"Ettersende dokumentasjon\", \"Sjekke/endre kontonr\"]\n",
    "#NUM_LABELS = len(cols_to_use)\n",
    "#l_df = load_labeled_data(cols_to_use, old = False)\n",
    "#df, l_df = filter_on_labeled_data(df,l_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Størrelse data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_df.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lemmatize_data(df, preprocessed = True)\n",
    "inputExamples = createInputExamples(df, l_df, preprocessed = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_data2(inputExamples)\n",
    "train_loader, test_loader = createDataLoaders(train_data, test_data, l_df.columns.values, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertMultiLabler(t.BertConfig.from_json_file(CONFIG_PATH), isTrained = True)\n",
    "model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trener opp modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, o1 = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(train_loader, 0):\n",
    "        ids = data['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "        mask = data['masks'].to(DEVICE, dtype = torch.long)\n",
    "        token_type_ids = data['tokens'].to(DEVICE, dtype = torch.long)\n",
    "        targets = data['labels'].to(DEVICE, dtype = torch.float)\n",
    "        \n",
    "        outputs = model(ids, mask, token_type_ids)\n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        if _%10==0:\n",
    "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "#for epoch in range(EPOCHS):\n",
    "#    train(epoch)\n",
    "#Trener litt mer - dog må siste klassifiseringslaget trenes helt opp på nytt igjen.. :( \n",
    "for epoch in range(4,6):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.base_model.save_pretrained(\"../hjemmesnekk/multilingual-trained/26.08_16_labels_8+2_epochs_kun_BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validerer modellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   Since we have a multiclass, multilabling problem, metrics such as f1 is not straight forward. \n",
    "   Thus using both f1 and hamming distance:\n",
    "\"\"\"\n",
    "def conf_matrix(outputs, targets):\n",
    "    targets = np.array(targets) == 1 # Converting to true false matrix\n",
    "    assert outputs.shape == targets.shape\n",
    "    \n",
    "    tps, tns, fps, fns = 0,0,0,0\n",
    "    for i, doc in enumerate(outputs):\n",
    "        for j, guessed_topic in enumerate(doc):\n",
    "            if guessed_topic == targets[i][j] == 1:\n",
    "                tps += 1\n",
    "            elif guessed_topic == targets[i][j] == 0:\n",
    "                tns += 1\n",
    "            elif (guessed_topic == 1) and (targets[i][j] == 0):\n",
    "                fps += 1\n",
    "            elif (guessed_topic == 0) and (targets[i][j] == 1):\n",
    "                fns += 1\n",
    "            else:\n",
    "                raise TypeError(\"WTF\")\n",
    "    \n",
    "    \n",
    "    acc = (tps+tns)/(tps+tns+fps+fns)\n",
    "    prec = (tps)/(tps+fps)\n",
    "    rec = (tps)/(tps+fns)\n",
    "    f1 = 2*(prec*rec)/(prec+rec)\n",
    "    return {\n",
    "        \"accuracy\":acc,\n",
    "        \"precision\":prec,\n",
    "        \"recall\":rec,\n",
    "        \"f1\":f1\n",
    "    }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    \n",
    "    label_map = {i : label for i, label in enumerate(l_df.columns.values)}\n",
    "    texts = []\n",
    "    pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(test_loader, 0):\n",
    "            ids = data['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = data['masks'].to(DEVICE, dtype = torch.long)\n",
    "            token_type_ids = data['tokens'].to(DEVICE, dtype = torch.long)\n",
    "            targets = data['labels'].to(DEVICE, dtype = torch.float)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "            \n",
    "            #texts.extend(data['text'])\n",
    "            \n",
    "    return fin_outputs, fin_targets, texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1):\n",
    "    outputs, targets, texts = validate(epoch)\n",
    "    outputs = np.array(outputs) >= PRED_TRESH\n",
    "    res = conf_matrix(outputs, targets)\n",
    "    \n",
    "    print(f\"Accuracy: {np.round(res['accuracy'],4)} for epoch {epoch}\")\n",
    "    print(f'F1-score:{np.round(res[\"f1\"],4)} for epoch: {epoch}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicte helt nye instanser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(pred_texts: list(), labels, tokenizer):\n",
    "    model.eval()\n",
    "    \n",
    "    label_map = {i : label for i, label in enumerate(labels)}\n",
    "    pred_data = texts_to_dataloader(pred_texts, tokenizer)\n",
    "        \n",
    "    texts = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(pred_data,0):\n",
    "            ids = data['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = data['masks'].to(DEVICE, dtype = torch.long)\n",
    "            tokens = data['tokens'].to(DEVICE, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask, tokens)\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach()\n",
    "            preds = (np.array(outputs) >= 0.2)[0]\n",
    "            inds = np.where(preds == True)[0].tolist()\n",
    "            \n",
    "            print(data['text'])\n",
    "            print(preds)\n",
    "            print(inds)\n",
    "            print(type(inds))\n",
    "            for ind in inds:\n",
    "                print(ind, label_map.get(ind))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Hei jeg ønsker å endre kontonr mitt.\",\"Hei jeg vil lage cv etterpå\", \"Jeg har blit permitert\", \"Ønsker å laste opp vedlegg\", \"Ønsker å lage CV og ettersende dokumentasjon\"]\n",
    "predict(texts, l_df.columns.values, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predikerer alle tekstene i dataen, og lagrer til pickle fil:\n",
    "\n",
    "- Denne metoden er ganske treig per nå, bruker ish 3s / tekst grunnet mye i minnet og med batch-size 1\n",
    "- Prøver å øke batch size for å se hvordan den presterer da, ev. med 1 batch (trenger ikke padding da)\n",
    "- Bør også gjøres med flere prosesser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_all_texts(df, labels, tokenizer, batch_size = 1) -> pd.DataFrame:\n",
    "    model.eval()\n",
    "    \n",
    "    df = df.iloc[0:200,:]\n",
    "    \n",
    "    pred_texts = df[\"raw_text\"].apply(lambda x: lemmatize(x)).values\n",
    "    label_map = {i : label for i, label in enumerate(labels)}\n",
    "    pred_data = texts_to_dataloader(pred_texts, tokenizer, batch_size = batch_size)\n",
    "        \n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(pred_data,0)):\n",
    "            ids = data['input_ids'].to(DEVICE, dtype = torch.long)\n",
    "            mask = data['masks'].to(DEVICE, dtype = torch.long)\n",
    "            tokens = data['tokens'].to(DEVICE, dtype = torch.long)\n",
    "            \n",
    "            outputs = model(ids, mask, tokens)\n",
    "            outputs = torch.sigmoid(outputs).cpu().detach()\n",
    "            preds = (np.array(outputs) >= 0.2)[0]\n",
    "            inds = np.where(preds == True)[0].tolist()\n",
    "            \n",
    "            predictions.append([str(label_map.get(ind)) for ind in inds])\n",
    "    \n",
    "    \n",
    "    assert len(pred_texts) == len(predictions)\n",
    "    \n",
    "    df[\"Prediction\"] = predictions\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def save_predicted_df(df):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df = predict_all_texts(df,l_df.columns.values, tokenizer, batch_size = 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lagrer predicted df:\n",
    "predicted_df_fixed = predicted_df.copy().drop(\"sentences\", axis = 1)\n",
    "predicted_df_fixed.to_csv(\"../hjemmesnekk/predicted_data/predicted_1833_instances_26.08.csv\")\n",
    "predicted_df_fixed.to_pickle(\"../hjemmesnekk/predicted_data/predicted_1833_instances_26_08.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualiserer resultatene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datapakke_custom import classifier_datapakke, convert_to_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = classifier_datapakke(predicted_df_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_monthly_piechart = cd.pie_chart_co_occuring_labels(min_Antall = 1)\n",
    "fig_monthly_piechart.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ = cd._df_labels\n",
    "succ  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
